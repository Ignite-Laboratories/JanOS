package see

// Irrationality
/*
# " 洧냪洧洧洧녩洧노洧녰洧녶洧녵洧녩洧녳洧녰洧노洧녽 "

洧노洧녰洧녵洧녽 has a unique take on irrationality, to say the least.

What makes something "irrational"?  Simply put, it's a number that cannot be represented as a fraction with whole
numbers in the numerator and denominator.  This very quality causes the number to infinitely repeat off with
no periodic repetition.  Mathematicians must -prove- irrationality, as they can't simply say "hey look, it
keeps on going!" and be taken seriously by their colleagues.

Computers, on the other hand, don't care whether something IS irrational. They don't care at all!  Being heartless
chunks of silicone, and all...Only their humans care about proving the chain of irrationality. The computer would
happily keep calculating an infinite number of placeholders if you let it, after all.

A computer isn't going to ask its teacher to grade its irrationality proof, though it could model such a construct.
Instead, it's going to OBSERVE the calculation appearing "irrational" while yielding the symbolic trail it used to
derive that result.  One could model a system that "fact checks" the symbolic trail emitted by the chain of calculation,
but the resulting numbers would always be considered as "irrational" when OBSERVED to be so.

To put it in G칬del's terms - there will always be statements about natural numbers that are true, but that are
unprovable within the system.

洧노洧녰洧녵洧녽 leverages this to create irrationality 'lenses' through which to perceive numbers.  All realized numbers are
dynamically generated to explicit precision on-demand - meaning the quality of irrationality is observed by every
step of the calculation, with no loss of precision in approximating the value.  If another calculation takes an
irrational operand in, it also gets the full-width irrational - even if requesting it to different precision
than the last activation.

	tl;dr - precision is carried with every operation, preserving irrationality

*/
type Irrationality byte
